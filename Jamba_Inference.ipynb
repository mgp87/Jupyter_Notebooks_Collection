{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgp87/Jupyter_Notebooks_Collection/blob/main/Jamba_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq transformers>=4.39.0 mamba-ssm causal-conv1d>=1.2.0 accelerate bitsandbytes --progress-bar off\n",
        "!pip install flash-attn --no-build-isolation\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Load model in 8-bit precision\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_skip_modules=[\"mamba\"]\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"ai21labs/Jamba-v0.1\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")"
      ],
      "metadata": {
        "id": "codgxb9q9abD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOlHBNBnqxuT"
      },
      "outputs": [],
      "source": [
        "# Tokenize input\n",
        "prompt = \"\"\"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? A. dry palms B. wet palms C. palms covered with oil D. palms covered with lotion. Answer:\"\"\"\n",
        "input_ids = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors='pt'\n",
        ").to(model.device)[\"input_ids\"]\n",
        "\n",
        "# Generate answer\n",
        "outputs = model.generate(input_ids, max_new_tokens=216)\n",
        "\n",
        "# Print output\n",
        "print(tokenizer.batch_decode(outputs))"
      ]
    }
  ]
}