{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h2KhHZGeMEL",
        "outputId": "a56239bf-20fb-4a3c-f23e-49548539e723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9CB7-esdqbB",
        "outputId": "f48e97c0-00be-495f-a937-51cd42d519e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
            "<|user|>\n",
            "How many helicopters can a human eat in one sitting?</s>\n",
            "<|assistant|>\n",
            "I do not have access to the specific details of human nutrition or body chemistry. However, it is generally believed that humans cannot consume more than about 500 calories in one sitting. This is because the human body has a limited capacity to store and utilize energy. A single helicopter is typically equipped with multiple engines and a large fuel tank, making it a heavy and bulky aircraft that requires a significant amount of fuel to operate. As a result, humans have been known to consume more than one helicopter in one sitting during special events or as a part of a prank or practical joke.<|system|>\n",
            "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
            "<|user|>\n",
            "How many helicopters can a human eat in one sitting?</s>\n",
            "<|assistant|>\n",
            "I do not have access to specific data or information about human physiology. However, the number of helicopters a human can consume in one sitting is not known for sure. It is generally believed that humans can consume a certain number of calories and energy before feeling full. However, the precise amount of calories or energy that can be consumed by a human in one sitting is not well established. It's always better to check with a healthcare professional or consult with a nutritionist for guidance on the appropriate amount of calories or energy for human consumption.<|system|>\n",
            "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
            "<|user|>\n",
            "How many helicopters can a human eat in one sitting?</s>\n",
            "<|assistant|>\n",
            "I don't have information about a person's physical ability to consume food, but it is common for humans to eat small amounts of food in small portions throughout the day, and it is generally safe to consume a few servings of food at a time. However, eating a large quantity of food in one sitting, particularly in a short amount of time, may lead to feelings of fullness, nausea, and stomach cramps, and may increase the risk of overeating and poor health outcomes. It is best to eat a balanced and healthy diet that includes a variety of foods throughout the day.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "def load_model():\n",
        "    return pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "models = [load_model() for _ in range(3)]\n",
        "tokenizer = models[0].tokenizer\n",
        "\n",
        "# Use the tokenizer's chat template to format each message\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Generate responses using each of the three models and concatenate them\n",
        "responses = []\n",
        "for model in models:\n",
        "    outputs = model(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "    response = outputs[0]['generated_text']\n",
        "    responses.append(response)\n",
        "\n",
        "# Print out the final generated text\n",
        "final_output = ''.join(responses)\n",
        "print(final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key changes:\n",
        "\n",
        "Averaging Generated Text: Instead of averaging logits or probabilities, we directly average the generated text from each model.\n",
        "Token Counting: We count the occurrences of each token at each position in the responses.\n",
        "Most Frequent Token: We select the most frequent token at each position as the averaged token.\n",
        "This approach might not be as fine-grained as averaging logits or probabilities, but it can still help produce a more consistent and coherent output in cases where direct access to token probabilities is not possible."
      ],
      "metadata": {
        "id": "GCpBcWBEmSzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "def load_model():\n",
        "    return pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "models = [load_model() for _ in range(3)]\n",
        "tokenizer = models[0].tokenizer\n",
        "\n",
        "# Enhanced prompt engineering (unchanged)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate. Use pirate vocabulary and mannerisms in your replies.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting, matey?\"},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Ensemble generation with averaging (corrected)\n",
        "responses = []\n",
        "for model in models:\n",
        "    outputs = model(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "    response = outputs[0]['generated_text']\n",
        "    responses.append(response)\n",
        "\n",
        "# Average the generated text directly\n",
        "averaged_text = \"\"\n",
        "for i in range(min(len(response) for response in responses)):\n",
        "    token_counts = {}\n",
        "    for response in responses:\n",
        "        token = response[i]\n",
        "        token_counts[token] = token_counts.get(token, 0) + 1\n",
        "    most_frequent_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    averaged_token = most_frequent_tokens[0][0]  # Choose the most frequent token\n",
        "    averaged_text += averaged_token\n",
        "\n",
        "print(averaged_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_PLQL0hjNME",
        "outputId": "9a86b428-d295-41db-e893-8f86b9b31e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a friendly chatbot who always responds in the style of a pirate. Use pirate vocabulary and mannerisms in your replies.</s>\n",
            "<|user|>\n",
            "How many helicopters can a human eat in one sitting, matey?</s>\n",
            "<|assistant|>\n",
            "That's a tough one! A human can eae or und 250-300 calories of food per hour, depending on their body mmas index (BMI)r However,  t's safe to s y that ai av rage person can co sume around 1,000-1,500 calories gh a sihele sitting, so it's possible that you cthld eat more than a few helicopters in one sitting. It's alio important to note that differenthfoods have different caloric valuest so be sure to account\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "class DeviceAwareModelWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        attr = getattr(self.model, name)\n",
        "\n",
        "        if name == \"__call__\" and isinstance(self.model, tuple):\n",
        "            def wrapped_call(*args, **kwargs):\n",
        "                input_dict = {\"inputs\": {}}\n",
        "                inputs = {}\n",
        "                for key, value in zip(self.model[0].config.keys(), args[0][0]):\n",
        "                    inputs[key] = value\n",
        "                input_dict['inputs'] = inputs\n",
        "                output = attr(**input_dict)[0]['generated_text'][0]\n",
        "                return output\n",
        "\n",
        "            return wrapped_call\n",
        "\n",
        "        elif name == \"to\":\n",
        "            def moved_model_wrapper(*args, **kwargs):\n",
        "                new_model = self.model.half().to(args[0])\n",
        "                return DeviceAwareModelWrapper(new_model)\n",
        "\n",
        "            return moved_model_wrapper\n",
        "\n",
        "        return attr\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError(\"Please use the __call__ method directly.\")\n",
        "\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    model = pipe.model  # Access the underlying model\n",
        "    model = model.to(device)  # Move the model to the device\n",
        "    return model  # Return the moved model\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "models = [load_model() for _ in range(3)]  # Load moved models directly\n",
        "\n",
        "# Create a tokenizer instance\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to the same device\n",
        "models = [load_model().to(device) for _ in range(3)]\n",
        "# Do not move tokenizer to the device\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# Enhanced prompt engineering (unchanged)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate. Use pirate vocabulary and mannerisms in your replies.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting, matey?\"},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Ensemble generation with voting (improved)\n",
        "responses = []\n",
        "probs = []\n",
        "for model in models:\n",
        "    # Move input tensor to the same device\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids, max_length=256, min_length=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, return_dict_in_generate=True, output_scores=True)\n",
        "    response = tokenizer.decode(outputs[\"sequences\"][0])\n",
        "    prob = outputs[\"scores\"]\n",
        "    responses.append(response)\n",
        "    probs.append(prob)\n",
        "\n",
        "# Vote for the best token at each position\n",
        "for i in range(min(len(response) for response in responses)):\n",
        "    token_probs = {}\n",
        "    total_probs = torch.zeros(1)  # Initialize a zero tensor to accumulate probabilities\n",
        "    for j in range(len(models)):\n",
        "        input_ids = tokenizer.encode(prompt[:i+1], return_tensors=\"pt\").to(device)  # Update input_ids based on current context length\n",
        "        outputs = model.forward(input_ids)[0]  # Get logits from forward pass\n",
        "        for k, token in enumerate(responses[j][i]):\n",
        "            if token != ' ':  # Skip end-of-text tokens\n",
        "                token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "                token_logit = outputs[-1, k, token_id].unsqueeze(0)  # Extract logit corresponding to the target token\n",
        "                token_prob = torch.softmax(token_logit, dim=-1).detach().cpu().numpy()[0]  # Calculate probability\n",
        "                total_probs += token_prob  # Accumulate probability\n",
        "                token_probs[token] = token_probs.get(token, 0) + token_prob\n",
        "    best_tokens = sorted(token_probs.items(), key=lambda x: x[1], reverse=True)\n",
        "    if best_tokens:\n",
        "        voted_token = best_tokens[0][0]  # Choose the token with the highest average probability\n",
        "    else:\n",
        "        print(\"No valid votes found.\")\n",
        "        break\n",
        "    voted_text = \"\"\n",
        "    voted_text += voted_token\n",
        "\n",
        "print(voted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "fWvHr5MX7KrG",
        "outputId": "e4e595c2-a971-4872-8662-20acb6ff57aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "WARNING:accelerate.big_modeling:You shouldn't move a model when it is dispatched on multiple devices.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1695ccc8a981>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Load moved models directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Create a tokenizer instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1695ccc8a981>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Load moved models directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Create a tokenizer instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1695ccc8a981>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m  \u001b[0;31m# Access the underlying model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move the model to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;31m# Return the moved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't move a model that has some modules offloaded to cpu or disk.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
          ]
        }
      ]
    }
  ]
}