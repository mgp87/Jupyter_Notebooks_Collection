{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LiteLlama Model:**"
      ],
      "metadata": {
        "id": "hJqqCvhvMqoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EWy2qqEHP8S",
        "outputId": "dcd194e7-c5aa-48ce-f6ef-932860e7ec47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Please provide your personal definition of intelligence. Do you think that intelligence scales up with parameter count, why or why not?: I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam. I am a student of physics and I am studying for my physics exam.\n",
            "I am a student of physics\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_path = 'ahxt/LiteLlama-460M-1T'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "prompt = 'Q: Please provide your personal definition of intelligence. Do you think that intelligence scales up with parameter count, why or why not?:'\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "tokens = model.generate(input_ids, max_length=400)\n",
        "print( tokenizer.decode(tokens[0].tolist(), skip_special_tokens=True) )\n",
        "# Q: What is the largest bird?\\nA: The largest bird is a black-headed gull."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiny Llama Model**\n",
        "\n",
        "Tiny Llama has a little over 2x the parameters that LiteLlama has. It can take a while for Tiny Llama to generate tokens on CPU mode.\n",
        "\n",
        "Note: You should run the code, then restart the Runtime when it errors and tells you that accelerate needs to be installed."
      ],
      "metadata": {
        "id": "_PQHzc6sMyff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"Please provide your personal definition of intelligence. Do you think that intelligence scales up with parameter count, why or why not?\"},\n",
        "]\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])\n",
        "# <|system|>\n",
        "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
        "# <|user|>\n",
        "# How many helicopters can a human eat in one sitting?</s>\n",
        "# <|assistant|>\n",
        "# ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvex4KYbLtCr",
        "outputId": "b4e1ffb8-b101-43af-cc82-8e19b8d6e848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "<|system|>\n",
            "You are a friendly chatbot</s>\n",
            "<|user|>\n",
            "Please provide your personal definition of intelligence. Do you think that intelligence scales up with parameter count, why or why not?</s>\n",
            "<|assistant|>\n",
            "I do not have the ability to have personal opinions. However, intelligence can be defined in various ways, depending on the context. Some definitions suggest that intelligence refers to the ability to perform certain tasks or tasks effectively, such as learning a new language, solving complex mathematical problems, or recognizing faces. Others suggest that intelligence can be measured by the extent to which an individual can adapt to changing environments, think creatively, or solve problems.\n",
            "\n",
            "regarding the question, whether intelligence scales up with parameter count, it's difficult to say without knowing the specific context in which the question was asked. Intelligence can be learned and developed through education, training, and experience, which can lead to increased intelligence over time. However, it's also possible that the more complex the task or environment, the more difficult it is to acquire or develop intelligence, especially if the task or environment is highly specialized or requires exceptional skill. Ultimately, the answer depends on the context and the specific criteria being measured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiny Llama Additional Questions:**"
      ],
      "metadata": {
        "id": "Xsz9W3PmPO7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of California?\"},\n",
        "]\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3MUBQ-zOrgq",
        "outputId": "4568d1e7-c540-4db7-fa10-e21ed527279f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
            "<|user|>\n",
            "What is the capital of California?</s>\n",
            "<|assistant|>\n",
            "The capital of California is Sacramento.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly assistant\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"What should I do if a wild Llama is staring at me?\"},\n",
        "]\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E28Io4eYPHfu",
        "outputId": "e3be7e34-994e-4bce-ea87-0a51ceabe1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a friendly assistant</s>\n",
            "<|user|>\n",
            "What should I do if a wild Llama is staring at me?</s>\n",
            "<|assistant|>\n",
            "If a wild Llama is staring at you, it's usually due to fear or confusion. Here are a few suggestions to help the llama:\n",
            "\n",
            "1. Reassure the llama: Try to calm the llama by saying something like, \"It's okay, I'm not going to hurt you,\" or \"I'm just here to help you.\"\n",
            "\n",
            "2. Offer a treat: If possible, offer the llama a small treat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LiteLlama Additional Questions:**"
      ],
      "metadata": {
        "id": "vpwFDPUjPYWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "prompt = 'Q: What should I do if a wild Llama is staring at me?\\nA:'\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "tokens = model.generate(input_ids, max_length=75)\n",
        "print( tokenizer.decode(tokens[0].tolist(), skip_special_tokens=True) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwn99To1Nzlv",
        "outputId": "8af4f219-0f93-4377-f656-53a5df3e5289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What should I do if a wild Llama is staring at me?\n",
            "A: You should be able to see the Llama's eyes.\n",
            "A: You should be able to see the Llama's eyes.\n",
            "A: You should be able to see the Llama's eyes.\n",
            "A: You should be able to see the Llama's eyes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "prompt = 'Q: What is the capital of California?\\nA:'\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "tokens = model.generate(input_ids, max_length=75)\n",
        "print( tokenizer.decode(tokens[0].tolist(), skip_special_tokens=True) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD2TfGnBOUe2",
        "outputId": "525c2656-1572-420f-8c98-606c1a1d314c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the capital of California?\n",
            "A: California is the state's capital.\n",
            "B: California is the state's capital.\n",
            "C: California is the state's capital.\n",
            "D: California is the state's capital.\n",
            "E: California is the state's capital.\n",
            "F: California is the state's capital.\n",
            "G: California is the\n"
          ]
        }
      ]
    }
  ]
}