{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgp87/Jupyter_Notebooks_Collection/blob/main/transformerCourse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxp0tskDecQy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProdAttention(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "      super(ScaledDotProdAttention, self).__init__()\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2,-1)) / np.sqrt(query.size(-1))\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        return torch.matmul(attention, value), attention\n"
      ],
      "metadata": {
        "id": "wo8NWO1WsLvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.d_k = d_model // nhead\n",
        "        self.d_v = d_model // nhead\n",
        "\n",
        "        self.linear_q = nn.Linear(d_model, d_model)\n",
        "        self.linear_k = nn.Linear(d_model, d_model)\n",
        "        self.linear_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "        self.scaled_dot_prod_attention = ScaledDotProdAttention(dropout)\n",
        "\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None, key_padding_mask=None):\n",
        "\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        query = self.linear_q(query).view(batch_size, -1, self.nhead, self.d_k).transpose(1,2)\n",
        "        key = self.linear_q(key).view(batch_size, -1, self.nhead, self.d_k).transpose(1,2)\n",
        "        value = self.linear_q(value).view(batch_size, -1, self.nhead, self.d_v).transpose(1,2)\n",
        "\n",
        "        output, attention_scores = self.scaled_dot_prod_attention(query, key, value)\n",
        "\n",
        "        output_concat = output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        output_concat = self.linear_layer(output_concat)\n",
        "\n",
        "        return self.dropout(output_concat)"
      ],
      "metadata": {
        "id": "jUg3hEjNu-VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0,d_model,2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0,1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "EPJ5nDovyfT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoswiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_mlp=1024, dropout=0.1):\n",
        "        super(PoswiseFeedForward, self).__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(d_model, d_mlp)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_mlp, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = self.linear_1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.linear_2(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "R3eTyseV2iVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-5):\n",
        "        super(LayerNorm, self).__init__()\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      mean = x.mean(dim=-1, keepdim=True)\n",
        "      std = x.std(dim=-1, keepdim=True)\n",
        "\n",
        "      x = (x - mean) / (std + self.eps)\n",
        "      x = self.gamma * x + self.beta\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "1UKS-WHw3axJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_mlp, dropout=0.1):\n",
        "      super(EncoderBlock, self).__init__()\n",
        "\n",
        "\n",
        "      self.multi_head_attention = MultiHeadAttention(d_model, nhead, dropout)\n",
        "\n",
        "      self.feed_forward = PoswiseFeedForward(d_model, d_mlp, dropout)\n",
        "\n",
        "      self.layer_norm1 = LayerNorm(d_model)\n",
        "      self.layer_norm2 = LayerNorm(d_model)\n",
        "\n",
        "      self.dropout1 = nn.Dropout(dropout)\n",
        "      self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
        "\n",
        "        x2 = self.multi_head_attention(x, x, x, mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
        "\n",
        "        x2 = self.layer_norm1(x2)\n",
        "\n",
        "        x = x + self.dropout1(x2)\n",
        "\n",
        "        x2 = self.feed_forward(x)\n",
        "\n",
        "        x2 = self.layer_norm2(x2)\n",
        "\n",
        "        x = x + self.dropout2(x2)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "DOH3i1cl4kzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_mlp, dropout=0.1):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "\n",
        "        self.masked_multi_head_attention = MultiHeadAttention(d_model, nhead, dropout)\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        self.feed_forward = PoswiseFeedForward(d_model, d_mlp, dropout)\n",
        "\n",
        "        self.layer_norm1 = LayerNorm(d_model)\n",
        "        self.layer_norm2 = LayerNorm(d_model)\n",
        "        self.layer_norm3 = LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "\n",
        "        tgt2 = self.masked_multi_head_attention(tgt,tgt,tgt, mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
        "\n",
        "        tgt2 = self.layer_norm1(tgt2)\n",
        "\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "\n",
        "        tgt2 = self.multi_head_attention(tgt2, memory, memory, mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
        "\n",
        "        tgt2 = self.layer_norm2(tgt2)\n",
        "\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "\n",
        "        tgt2 = self.feed_forward(tgt)\n",
        "\n",
        "        tgt2 = self.layer_norm3(tgt2)\n",
        "\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "\n",
        "        return tgt\n",
        "\n"
      ],
      "metadata": {
        "id": "qulwFIzA7bBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, d_model, nhead, n_encoder, n_decoder, d_mlp, max_len, vocab_size, pad_idx, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        encoder_layer = EncoderBlock(d_model, nhead, d_mlp, dropout)\n",
        "        encoder_norm = LayerNorm(d_model)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, n_encoder, encoder_norm)\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layer = DecoderBlock(d_model, nhead, d_mlp, dropout)\n",
        "        decoder_norm = LayerNorm(d_model)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, n_decoder, decoder_norm)\n",
        "\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)\n",
        "\n",
        "        # Embedding layers for input and output\n",
        "        self.input_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.output_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "\n",
        "        # Final linear layer\n",
        "        self.linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, src, output, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, is_causal=False):\n",
        "\n",
        "        src = self.input_embedding(src) * np.sqrt(self.d_model)\n",
        "\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        encoder_outputs = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "\n",
        "        output = self.output_embedding(output) * np.sqrt(self.d_model)\n",
        "\n",
        "        output = self.pos_encoder(output)\n",
        "\n",
        "        decoder_outputs = self.decoder(output, encoder_outputs, tgt_mask=tgt_mask, memory_mask=src_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        outputs = self.linear(decoder_outputs)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "jNFbS0trG0Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "nhead = 1\n",
        "n_encoder_layers = 1\n",
        "n_decoder_layers = 1\n",
        "d_mlp = 1024\n",
        "max_len = 6\n",
        "vocab_size = len(list(\"abcdefghijklmnop\"))\n",
        "pad_idx = 0\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "rlwbw2HuKlkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel(d_model, nhead, n_encoder_layers, n_decoder_layers, d_mlp, max_len, vocab_size, pad_idx, dropout)"
      ],
      "metadata": {
        "id": "gSKYgU6gK9wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "cBGgBr1MLK_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseDataset(Dataset):\n",
        "    def __init__(self, length=10000, seq_len=10):\n",
        "\n",
        "        self.length = length\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab = list(\"abcdefghijklmnop\")\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.randint(high=self.vocab_size, size=(self.seq_len,))\n",
        "        return sequence, torch.flip(sequence, dims=[0])\n"
      ],
      "metadata": {
        "id": "xE6FToaysY2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ReverseDataset(seq_len=max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=5, shuffle=True)"
      ],
      "metadata": {
        "id": "1sqemawJuPN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "model = TransformerModel(d_model, nhead, n_encoder_layers, n_decoder_layers, d_mlp, max_len, vocab_size, pad_idx, dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "v-F6DOMfu2qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_text(tokens, dataset):\n",
        "    return ''.join(dataset.idx_to_char[token.item()] for token in tokens)"
      ],
      "metadata": {
        "id": "NURfqFeMvQCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = next(iter(dataloader))\n",
        "print(\"Input: \", tokens_to_text(inputs[4], dataset))\n",
        "print(\"Target: \", tokens_to_text(targets[4], dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr9aJYG1wCaF",
        "outputId": "b1c7c1ef-984f-44e1-e478-17ca95c63f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  kmkbpb\n",
            "Target:  bpbkmk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (input, target) in enumerate(dataloader):\n",
        "\n",
        "        input = input.T.to(device)\n",
        "        target = target.T.to(device)\n",
        "\n",
        "        taget_input = target[:-1, :]\n",
        "        target_real = target[1:, :]\n",
        "\n",
        "        output = model(input, target_real)\n",
        "\n",
        "        loss = criterion(output.view(-1, vocab_size), target_real.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "dviG3TGuwb3G",
        "outputId": "55f09e7d-ad13-4915-b960-a191f733b3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Iteration: 0, Loss: 2.484734296798706\n",
            "Epoch: 0, Iteration: 100, Loss: 0.4349328875541687\n",
            "Epoch: 0, Iteration: 200, Loss: 0.14970877766609192\n",
            "Epoch: 0, Iteration: 300, Loss: 0.06762726604938507\n",
            "Epoch: 0, Iteration: 400, Loss: 0.03968893364071846\n",
            "Epoch: 0, Iteration: 500, Loss: 0.029801957309246063\n",
            "Epoch: 0, Iteration: 600, Loss: 0.022669054567813873\n",
            "Epoch: 0, Iteration: 700, Loss: 0.01714584417641163\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-7a5becae2872>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def output_to_text(output, dataset):\n",
        "\n",
        "    tokens = F.softmax(output, dim=-1)\n",
        "\n",
        "    tokens = torch.argmax(tokens, dim=-1)\n",
        "\n",
        "    text = ''.join(dataset.idx_to_char[token.item()] for token in tokens)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "UDzaaAZCzMUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = next(iter(dataloader))\n",
        "\n",
        "index = 1\n",
        "\n",
        "print(\"Input: \", tokens_to_text(inputs[index], dataset))\n",
        "print(\"Target: \", tokens_to_text(targets[index], dataset))\n",
        "\n",
        "input = inputs[index].T.to(device)\n",
        "target = targets[index].T.to(device)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtl5FBGt51ju",
        "outputId": "00746e46-9407-444f-a72a-415d9b6dc380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  hneobc\n",
            "Target:  cboenh\n",
            "tensor([ 2,  1, 14,  4, 13,  7], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(input, target)"
      ],
      "metadata": {
        "id": "uvOA_9FY6VaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input: \", tokens_to_text(inputs[index], dataset))\n",
        "print(\"Prediction: \", output_to_text(output[index], dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2wphQwD6Zj2",
        "outputId": "d0acb5e0-da36-4b44-f6c3-d2c7716cc655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  hneobc\n",
            "Prediction:  cboenh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2tc2wexI7BuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}