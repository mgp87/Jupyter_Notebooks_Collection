{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgp87/Jupyter_Notebooks_Collection/blob/main/GPTFast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq gptfast==0.2.1 --progress-bar off\n",
        "!pip install -U -qqq numpy==1.26.3 --progress-bar off"
      ],
      "metadata": {
        "id": "eSIGYsTbS_iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeIuGVwvS-1a",
        "outputId": "27e37979-f14d-4bf6-a28b-ba6fc05c5a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-xl\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1600,\n",
            "  \"n_head\": 25,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 48,\n",
            "  \"n_positions\": 1024,\n",
            "  \"output_past\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.39.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-xl\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1600,\n",
            "  \"n_head\": 25,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 48,\n",
            "  \"n_positions\": 1024,\n",
            "  \"output_past\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.39.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-xl\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1600,\n",
            "  \"n_head\": 25,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 48,\n",
            "  \"n_positions\": 1024,\n",
            "  \"output_past\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.39.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
            "The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/model.safetensors\n",
            "Instantiating GPT2LMHeadModel model under default dtype torch.float16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-xl.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2-xl/snapshots/15ea56dee5df4983c59b2538573817e1667135e2/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.39.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt fast eval time 0: 2.40134716796875\n",
            "gpt fast eval time 1: 2.846116943359375\n",
            "gpt fast eval time 2: 2.33012841796875\n",
            "gpt fast eval time 3: 2.32789990234375\n",
            "gpt fast eval time 4: 2.350912841796875\n",
            "gpt fast eval time 5: 2.40123291015625\n",
            "gpt fast eval time 6: 2.797267333984375\n",
            "gpt fast eval time 7: 2.3549501953125\n",
            "gpt fast eval time 8: 2.338822265625\n",
            "gpt fast eval time 9: 2.33497216796875\n",
            "~~~~~~~~~~\n",
            "Write me a short story.\n",
            "\n",
            "I'm not sure if you've heard of it, but there's a story about a guy who's been living in a house for a while and he's been having a hard time finding a job. He's been living in a house\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from GPTFast.Core import gpt_fast\n",
        "from GPTFast.Helpers import timed\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "torch._dynamo.reset()\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def argmax_variation(\n",
        "    self, probabilities: torch.Tensor, temperature: float = 1, k: int = 5\n",
        "):\n",
        "    # Apply temperature scaling\n",
        "    device = probabilities.device\n",
        "    scaled_probabilities = probabilities / temperature\n",
        "\n",
        "    # Ensure k is within a valid range\n",
        "    k = min(k, probabilities.size(-1))\n",
        "\n",
        "    # Get the indices of the top-k scaled probabilities along the specified dimension\n",
        "    top_k_indices = torch.topk(scaled_probabilities, k, dim=-1).indices\n",
        "\n",
        "    # Generate random indices for sampling\n",
        "    random_indices = torch.randint(0, k, (1,) * probabilities.dim()).to(device)\n",
        "\n",
        "    # Use gathered indices to get the final sampled token\n",
        "    sampled_token = top_k_indices.gather(-1, random_indices).to(device)\n",
        "\n",
        "    return sampled_token.unsqueeze(0)\n",
        "\n",
        "\n",
        "def argmax(self, probabilities):\n",
        "    # Use argmax to get the token with the maximum probability\n",
        "    max_prob_index = torch.argmax(probabilities, dim=-1)\n",
        "    return max_prob_index.view(1, 1)\n",
        "\n",
        "\n",
        "model_name = \"gpt2-xl\"\n",
        "draft_model_name = \"gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "initial_string = \"Write me a short story.\"\n",
        "input_tokens = tokenizer.encode(initial_string, return_tensors=\"pt\").to(device)\n",
        "\n",
        "N_ITERS = 10\n",
        "MAX_TOKENS = 50\n",
        "\n",
        "cache_config = {\n",
        "    \"model_config\": {\n",
        "        \"path_to_blocks\": [\"transformer\", \"h\"],\n",
        "        \"child_ref_in_parent_forward\": [\"transformer\", \"block\"],\n",
        "    },\n",
        "    \"block_config\": {\n",
        "        \"path_to_attn\": [\"attn\"],\n",
        "        \"child_ref_in_parent_forward\": [\"attn\"],\n",
        "    },\n",
        "    \"attn_config\": {\n",
        "        \"cache_update_config\": {\n",
        "            \"kv_cache_condition\": \"if layer_past is not None\",\n",
        "            \"key_name\": \"key\",\n",
        "            \"value_name\": \"value\",\n",
        "        },\n",
        "        \"causal_mask_config\": {\n",
        "            \"causal_mask_application\": \"conditional\",\n",
        "            \"causal_mask_method\": \"_attn\",\n",
        "            \"causal_mask_condition\": \"not self.is_cross_attention\",\n",
        "        },\n",
        "    },\n",
        "    \"imports\": [\n",
        "        \"import torch\",\n",
        "        \"import transformers\",\n",
        "        \"from transformers import *\",\n",
        "        \"from torch import *\",\n",
        "        \"from typing import *\",\n",
        "        \"import types\",\n",
        "        \"from transformers.modeling_outputs import *\",\n",
        "        \"from torch import nn\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "gpt_fast_model = gpt_fast(\n",
        "    model_name,\n",
        "    sample_function=argmax,\n",
        "    max_length=60,\n",
        "    cache_config=cache_config,\n",
        "    draft_model_name=draft_model_name,\n",
        ")\n",
        "gpt_fast_model.to(device)\n",
        "\n",
        "fast_compile_times = []\n",
        "for i in range(N_ITERS):\n",
        "    with torch.no_grad():\n",
        "        res, compile_time = timed(\n",
        "            lambda: gpt_fast_model.generate(\n",
        "                cur_tokens=input_tokens, max_tokens=MAX_TOKENS, speculate_k=6\n",
        "            )\n",
        "        )\n",
        "    fast_compile_times.append(compile_time)\n",
        "    print(f\"gpt fast eval time {i}: {compile_time}\")\n",
        "print(\"~\" * 10)\n",
        "\n",
        "print(tokenizer.decode(res[0]))"
      ]
    }
  ]
}